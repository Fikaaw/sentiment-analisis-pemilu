{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7398781,"sourceType":"datasetVersion","datasetId":4301986}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATASET","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/dataset-pemilu-2/Dataset Pemilu 2 by crawling.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATA PREPROCESSING ","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df['datetime']\ndel df['username']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CLEANING","metadata":{}},{"cell_type":"code","source":"df.drop_duplicates(subset =\"comments\", keep = 'first', inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\nimport nltk\n\n# Fungsi untuk menghapus URL\ndef remove_URL(tweet):\n    if tweet is not None and isinstance(tweet, str):\n        url = re.compile(r'https?://\\S+|www\\.\\S+')\n        return url.sub(r'', tweet)\n    else:\n        return tweet\n\n# Fungsi untuk menghapus HTML\ndef remove_html(tweet):\n    if tweet is not None and isinstance(tweet, str):\n        html = re.compile(r'<.*?>')\n        return html.sub(r'', tweet)\n    else:\n        return tweet\n\n# Fungsi untuk menghapus emoji\ndef remove_emoji(tweet):\n    if tweet is not None and isinstance(tweet, str):\n        emoji_pattern = re.compile(\"[\"\n            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n            u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n            u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n            u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n            u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n            u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n            u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n            u\"\\U0001F004-\\U0001F0CF\"  # Additional emoticons\n            u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n                               \"]+\", flags=re.UNICODE)\n        return emoji_pattern.sub(r'', tweet)\n    else:\n        return tweet\n\n# Fungsi untuk menghapus simbol\ndef remove_symbols(tweet):\n    if tweet is not None and isinstance(tweet, str):\n        tweet = re.sub(r'[^a-zA-Z0-9\\s]', '', tweet)  # Menghapus semua simbol\n    return tweet\n\n# Fungsi untuk menghapus angka\ndef remove_numbers(tweet):\n    if tweet is not None and isinstance(tweet, str):\n        tweet = re.sub(r'\\d', '', tweet)  # Menghapus semua angka\n    return tweet\n\ndf['cleaning'] = df['comments'].apply(lambda x: remove_URL(x))\ndf['cleaning'] = df['cleaning'].apply(lambda x: remove_html(x))\ndf['cleaning'] = df['cleaning'].apply(lambda x: remove_emoji(x))\ndf['cleaning'] = df['cleaning'].apply(lambda x: remove_symbols(x))\ndf['cleaning'] = df['cleaning'].apply(lambda x: remove_numbers(x))\n\ndf.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CASE FOLDING","metadata":{}},{"cell_type":"code","source":"def case_folding(text):\n    if isinstance(text, str):\n        lowercase_text = text.lower()\n        return lowercase_text\n    else:\n        return text\n\ndf['case_folding'] = df['cleaning'].apply(case_folding)\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TOKENIZATION ","metadata":{}},{"cell_type":"code","source":"# Fungsi untuk tokenisasi\ndef tokenize(text):\n    if isinstance(text, str):\n        tokens = text.split()\n        return tokens\n    else:\n        return []\n\ndf['tokenize'] = df['case_folding'].apply(tokenize)\ndf.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FILTERING/ STOPWORD REMOVAL","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nnltk.download('stopwords')\nstop_words = stopwords.words('indonesian')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_stopwords(text):\n    return [word for word in text if word not in stop_words]\n\ndf['stopword removal'] = df['tokenize'].apply(lambda x: remove_stopwords(x))\n\ndf.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# STEMMING ","metadata":{}},{"cell_type":"code","source":"!pip install Sastrawi\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"factory = StemmerFactory()\nstemmer = factory.create_stemmer()\n\ndef stem_text(text):\n    return [stemmer.stem(word) for word in text]\n\ndf['stemming_data'] = df['stopword removal'].apply(lambda x: ' '.join(stem_text(x)))\n\ndf.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Menghapus baris yang mengandung nilai kosong\ndf_cleaned = df.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cleaned.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cleaned.to_csv('Hasil_Preprocessing_Data2.csv',encoding='utf8', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CLASSIFICATION","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport nltk\nimport re\nimport numpy as np\nimport gensim\nimport torch\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel, AdamW\nfrom transformers import pipeline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[df['stemming_data'].apply(lambda tokens: len(tokens) > 0)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roberta_classifier = pipeline('sentiment-analysis', model=\"ayameRushia/roberta-base-indonesian-1.5G-sentiment-analysis-smsa\",tokenizer=\"ayameRushia/roberta-base-indonesian-1.5G-sentiment-analysis-smsa\")\ndf['roberta_label'] = df['stemming_data'].apply(lambda x: roberta_classifier(x[:512])[0]['label'])  # Truncate to 512 tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['roberta_label'] = df['roberta_label'].astype('category')\nlabel_mapping = dict(enumerate(df['roberta_label'].cat.categories))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('Hasil_Labeling.csv',encoding='utf8', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working\nfrom IPython.display import FileLink\nFileLink('Hasil_Labeling.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Splitting dan CNN Model\n\nmax_length = max(len(seq) for seq in df['stemmed_tokens'])\nX_pad = pad_sequences(df['stemmed_tokens'], maxlen=max_length, padding='post', truncating='post', value='0', dtype=object)\nX = X_pad\ny = df['roberta_label']\ny","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Assuming df['roberta_label'] is already created\n\n# Add 'comments' column for context\ndf['comments'] = df['comments'].apply(lambda x: x[:512])  # Truncate to 512 tokens\n\n# Save relevant columns to CSV in Google Drive\ndrive_path = '/content/drive/MyDrive'  # Adjust this path based on your Google Drive folder structure\ncsv_file_path = os.path.join(drive_path, 'roberta_labels.csv')\ndf[['comments', 'roberta_label']].to_csv(csv_file_path, index=False)\ncsv_file_path_1 = os.path.join(drive_path, 'Dataclean.csv')\ndf.to_csv(csv_file_path, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# WORD EMBEDDING","metadata":{}},{"cell_type":"code","source":"%%time\n\ntokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) # tokenizing \n\nmodel_w2v = gensim.models.Word2Vec(\n            tokenized_tweet,\n            size=200, # desired no. of features/independent variables\n            window=5, # context window size\n            min_count=2, # Ignores all words with total frequency lower than 2.                                  \n            sg = 1, # 1 for skip-gram model\n            hs = 0,\n            negative = 10, # for negative sampling\n            workers= 32, # no.of cores\n            seed = 34\n) \n\nmodel_w2v.train(tokenized_tweet, total_examples= len(combi['tidy_tweet']), epochs=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}